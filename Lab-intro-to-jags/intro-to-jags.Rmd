```{r jags-setup, include=FALSE, purl=FALSE}
removefiles <- c()
knitr::opts_knit$set(unnamed.chunk.label = "jags-")
```


# JAGS for Bayesian time series analysis  {#chap-jags}
\chaptermark{JAGS}

In this lab, we will illustrate how to use JAGS to fit time series models with Bayesian methods. The purpose of this chapter is to teach you some basic JAGS models. To go beyond these basics, study the wide variety of software tools to do time series analysis using Bayesian methods, e.g. packages listed on the R Cran [TimeSeries](http://cran.r-project.org/web/views/TimeSeries.html) task view.   


### Data and pacakges {-}

For data for this lab, we will include a dataset on air quality in New York. We will load the data and create a couple new variables for future use. For the majority of our models, we are going to treat wind speed as the response variable for our time series models. 

```{r jags-loaddata, echo=TRUE, results='hide', eval=TRUE}
data(airquality, package = "datasets")
Wind <- airquality$Wind # wind speed
Temp <- airquality$Temp # air temperature
N <- dim(airquality)[1] # number of data points
```

To run this code, you will need to install JAGS for your operating platform using the instructions [here](http://sourceforge.net/projects/mcmc-jags/files/).  Click on JAGS, then the most recent folder, then the platform of your machine.  You will also need the **coda**, **rjags** and **R2jags** packages.
```{r jags-loadpackages, results='hide', message=FALSE, warnings=FALSE}
library(coda)
library(rjags)
library(R2jags)
```

## Overview {#sec-jags-overview}

In this chapter, we will be working up to simple univariate state-space JAGS models. We will write each of the models with the same univariate state-space form.

\begin{equation}
\begin{gathered}
x_t = b x_{t-1} + u + w_t, \, w_t \sim N(0,q)\\
y_t = x_t + a + v_t, \, v_t \sim \N(0, r)
\end{gathered}
(\#eq:jags-marss)
\end{equation}

We will be fitting linear regressions with this form, and this will mean the JAGS code is more verbose than necessary, but the goal is to build up to our univariate state-space code by building off simpler models.

## Linear regression with no covariates {#sec-jags-lr-no-covariates}

We will start with a linear regression with only an intercept, so that the predicted values of all observations are the same. We will write the model in the form of Equation \@ref(eq:jags-marss). Our model is
\begin{equation}
\begin{gathered}
x_t = u \\
y_t = x_t + v_t, v_t \sim \N(0, r)
\end{gathered}
(\#eq:jags-lr1)
\end{equation}
An equivalent way to think about this model is 
\begin{equation}
y_t \sim \N(E[y_t], r)
\end{equation}
$E[{y}_{t}] = x_t$ where $x_t = u$. 
In this linear regression model, we will treat the residual error as independent and identically distributed Gaussian observation error.

To run the JAGS model, we will need to start by writing the model in JAGS notation.  We can construct the model in Equation \@ref(eq:jags-lr1) as

```{r jags-lr1, results='hide', cache=TRUE}
# LINEAR REGRESSION with no covariates
# intercept only. The parameters are

model.loc <- "lm_intercept.txt" # name of the txt file
jagsscript <- cat("
model {  
   # priors on parameters
   u ~ dnorm(0, 0.01);
   inv.r ~ dgamma(0.001,0.001); # This is inverse gamma
   r <- 1/inv.r; # r is treated as derived parameter
   for(i in 1:N) {
      X[i] <- u
      EY[i] <- X[i];
      Y[i] ~ dnorm(EY[i], inv.r); 
   }
}  
", file = model.loc)
```
```{r clean1, echo=FALSE, purl=FALSE}
removefiles <- c(removefiles, model.loc)
```

A couple things to notice: JAGS is not vectorized so we need to use for loops (instead of matrix multiplication) and the `dnorm()` notation means that we assume that the value on the left is normally distributed around a particular mean with a particular precision (1 over the variance). `inv.r` is $1/r$.

The model can briefly be summarized as follows: there are two parameters in the model (the mean and $r$, the variance of the observation error). In JAGS instead of specifying the normal distribution with the variance, you pass in the precision (1/variance). Thus `dnorm(0, 0.01)`, our prior on $u$, is pretty vague. The precision receives a gamma prior, which is equivalent to the variance receiving an inverse gamma prior (fairly common for standard Bayesian regression models). Finally, we write a model for the data $y_t$ (`Y[i]`). Again we use the `dnorm()` distribution to say that the data are normally distributed (equivalent to our likelihood). 

The function from the **R2jags** package that we use to run the model is `jags()`. There is a parallel version of the function called `jags.parallel()` which is useful for larger, more complex models. The details of both can be found with `?jags` or `?jags.parallel`.

To run the model, we need to create several new objects, representing (1) a list of data that we will pass to JAGS, (2) a vector of parameters that we want to monitor and have returned back to R, and (3) the name of our text file that contains the JAGS model we wrote above. With those three things, we can call the `jags()` function. 

```{r jags-call-fun1, results='hide', cache=TRUE}
jags.data <- list("Y" = Wind, "N" = N) # named list of inputs
jags.params <- c("r", "u") # parameters to be monitored
mod_lm_intercept <- R2jags::jags(jags.data,
  parameters.to.save = jags.params,
  model.file = model.loc, n.chains = 3, n.burnin = 5000,
  n.thin = 1, n.iter = 10000, DIC = TRUE
)
```

Notice that the `jags()` function contains a number of other important arguments. In general, larger is better for all arguments: we want to run multiple MCMC chains (maybe 3 or more), and have a burn-in of at least 5000. The total number of samples after the burn-in period is n.iter-n.burnin, which in this case is 5000 samples. Because we are doing this with 3 MCMC chains, and the thinning rate equals 1 (meaning we are saving every sample), we will retain a total of 1500 posterior samples for each parameter.  

The saved object storing our model diagnostics can be accessed directly, and includes some useful summary output.
```{r jags-lm1-mod}
mod_lm_intercept
```

The last two columns in the summary contain `Rhat` (which we want to be close to 1.0), and `neff` (the effective sample size of each set of posterior draws). To examine the output more closely, we can pull all of the results directly into R,

```{r jags-lm1-attach, eval=FALSE}
R2jags::attach.jags(mod_lm_intercept)
```
Attaching the **R2jags** object loads the posteriors for the parameters and we can call them directly, e.g. `u`. If we don't want to attach them to our workspace, we can find the posteriors buried within the model object.
```{r jags-lm1-attach2}
post.params <- mod_lm_intercept$BUGSoutput$sims.list
```

We make a histogram of the posterior distributions of the parameters `u` and `r` with the following code,

```{r jags-plot-lm1, echo=TRUE, eval=TRUE, fig.show='hide'}
# Now we can make plots of posterior values
par(mfrow = c(2, 1))
hist(post.params$u, 40, col = "grey", xlab = "u", main = "")
hist(post.params$r, 40, col = "grey", xlab = "r", main = "")
```

(ref:jags-plot-hist-post) Plot of the posteriors for the linear regression model.

```{r jags-plot-hist-post, fig=TRUE, echo=FALSE, fig.width=6, fig.height=6, fig.cap='(ref:jags-plot-hist-post)'}
par(mfrow = c(2, 1))
hist(post.params$u, 40, col = "grey", xlab = "u", main = "")
hist(post.params$r, 40, col = "grey", xlab = "r", main = "")
```
 

Finally, we can run some useful diagnostics from the **coda** package on this model output. We have written a small function to make the creation of a MCMC list (an argument required for many of the diagnostics). The function is

```{r jags-lm1-mcmclist-func, cache=TRUE}
createMcmcList <- function(jagsmodel) {
  McmcArray <- as.array(jagsmodel$BUGSoutput$sims.array)
  McmcList <- vector("list", length = dim(McmcArray)[2])
  for (i in 1:length(McmcList)) McmcList[[i]] <- as.mcmc(McmcArray[, i, ])
  McmcList <- mcmc.list(McmcList)
  return(McmcList)
}
```

Creating the  MCMC list preserves the random samples generated from each chain and allows you to extract the samples for a given parameter (such as $\mu$) from any chain you want. To extract $\mu$ from the first chain, for example, you could use the following code. Because `createMcmcList()` returns a list of **mcmc** objects, we can summarize and plot these directly. Figure \@ref(fig:jags-plot-myList) shows the plot from `plot(myList[[1]])`.

```{r jags-make-myList, fig.show='hide'}
myList <- createMcmcList(mod_lm_intercept)
summary(myList[[1]])
plot(myList[[1]])
```

(ref:jags-plot-myList) Plot of an object output from $\texttt{creatMcmcList}$.

```{r jags-plot-myList,fig=TRUE, echo=FALSE, fig.width=6, fig.height=6, fig.cap='(ref:jags-plot-myList)'}
plot(myList[[1]])
```
 
For more quantitative diagnostics of MCMC convergence, we can rely on the **coda** package in R. There
are several useful statistics available, including the Gelman-Rubin diagnostic (for one or several chains), autocorrelation diagnostics (similar to the ACF you calculated above), the Geweke diagnostic, and Heidelberger-Welch test of stationarity. 

```{r jags-coda, results='hide', message=FALSE, warning=FALSE}
library(coda)
gelmanDiags <- coda::gelman.diag(createMcmcList(mod_lm_intercept), multivariate = FALSE)
autocorDiags <- coda::autocorr.diag(createMcmcList(mod_lm_intercept))
gewekeDiags <- coda::geweke.diag(createMcmcList(mod_lm_intercept))
heidelDiags <- coda::heidel.diag(createMcmcList(mod_lm_intercept))
```


## Linear regression with covariates {#sec-jags-covariates}

We can introduce `Temp` as the covariate explaining our response variable `Wind`. Our new equation is

\begin{equation}
\begin{gathered}
x_t = u + C\,c_t\\
y_t = x_t + v_t, v_t \sim \N(0, r)
\end{gathered}
(\#eq:jags-obs-eqn)
\end{equation}

We (1) modify the JAGS script to include a new coefficient---in this case `C`, (2) update the predictive equation to include the effects of the new covariate, and (3) we include the new covariate in our named data list. 

```{r jags-cov, results='hide', cache=TRUE}
# 1. LINEAR REGRESSION with covariates

model.loc <- ("lm_covariate.txt")
jagsscript <- cat("
model {  
   u ~ dnorm(0, 0.01); 
   C ~ dnorm(0,0.01);
   inv.r ~ dgamma(0.001,0.001); 
   r <- 1/inv.r; 
   
   for(i in 1:N) {
      X[i] <- u + C*c[i];
      EY[i] <- X[i]
      Y[i] ~ dnorm(EY[i], inv.r);
   }
}  
", file = model.loc)

jags.data <- list("Y" = Wind, "N" = N, "c" = Temp)
jags.params <- c("r", "EY", "u", "C")
mod_lm <- R2jags::jags(jags.data,
  parameters.to.save = jags.params,
  model.file = model.loc, n.chains = 3, n.burnin = 5000,
  n.thin = 1, n.iter = 10000, DIC = TRUE
)
```
```{r clean6, echo=FALSE, purl=FALSE}
removefiles <- c(removefiles, model.loc)
```


We can show the the posterior fits (the model fits) to the data. Here is a simple function whose arguments are one of our fitted models and the raw data. The function is: 

```{r jags-lm1ar-plot-func, results='hide'}
plotModelOutput <- function(jagsmodel, Y) {
  # attach the model
  EY <- jagsmodel$BUGSoutput$sims.list$EY
  x <- seq(1, length(Y))
  summaryPredictions <- cbind(
    apply(EY, 2, quantile, 0.025), apply(EY, 2, mean),
    apply(EY, 2, quantile, 0.975)
  )
  plot(Y,
    col = "white", ylim = c(min(c(Y, summaryPredictions)), max(c(Y, summaryPredictions))),
    xlab = "", ylab = "95% CIs of predictions and data", main = paste(
      "JAGS results:",
      jagsmodel$model.file
    )
  )
  polygon(c(x, rev(x)), c(summaryPredictions[, 1], rev(summaryPredictions[, 3])),
    col = "grey70", border = NA
  )
  lines(summaryPredictions[, 2])
  points(Y)
}
```

We can use the function to plot the predicted posterior mean with 95\% CIs, as well as the raw data. For example, try
```{r jags-lm1ar-plot, results='hide', eval=FALSE, fig.show='hide'}
plotModelOutput(mod_lm, Wind)
```


(ref:jags-lm1ar-plot1) Predicted posterior mean with 95\% CIs 

```{r jags-lm1ar-plot1, fig=TRUE, echo=FALSE, fig.width=6, fig.height=6, fig.cap='(ref:jags-lm1ar-plot1)', cache=FALSE}
plotModelOutput(mod_lm, Wind)
```


## Random walk with drift {#sec-jags-rw}

All of the previous three models are observation error only models. Switching gears, we can create process error models. We will start with a random walk model. In this model, the assumption is that the true state of nature (or latent states) are measured perfectly. Thus, all uncertainty is originating from process variation (for ecological problems, this is often interpreted as environmental variation). 

For this simple model, we will assume that our process of interest (in this case, daily wind speed) behaves as a random walk. We will call this process $x$ to prepare for the state-space model to come. We have no $y_t$ part of the equation in this case.

\begin{equation}
x_t = x_{t-1} + u + w_t, \text{ where }w_t \sim \N(0,q)
(\#eq:jags-obs-eqn-rw)
\end{equation}
Now $x_t$ is stochastic and $E[x_t] = x_{t-1} + u$ and $x_t \sim \N(E[x_t],q)$.

We are going to need to put a prior on $x_0$, which appears in $E[x_1]$. We could start with $t=2$ and skip this but we will start at $t=1$ since we will need to do that for later problems. The question is what prior should we put on $x_0$? This is not a stationary process. We will just put a vague prior on $x_0$.

The JAGS random walk model and R script to run it is below. We change the JAGS data to use `"X"` instead of `"Y"` since this model has only $x$.

```{r jags-ar1, results='hide', cache=TRUE}
# RANDOM WALK with drift

model.loc <- ("rw_intercept.txt")
jagsscript <- cat("
model {  
   u ~ dnorm(0, 0.01); 
   inv.q ~ dgamma(0.001,0.001); 
   q <- 1/inv.q;

   X0 ~ dnorm(0, 0.001);
   X[1] ~ dnorm(X0 + u, inv.q);
   for(i in 2:N) {
      X[i] ~ dnorm(X[i-1] + u, inv.q);
   }
}  
", file = model.loc)

jags.data <- list("X" = Wind, "N" = N)
jags.params <- c("q", "u")
mod_rw_intercept <- R2jags::jags(jags.data,
  parameters.to.save = jags.params, model.file = model.loc,
  n.chains = 3, n.burnin = 5000, n.thin = 1, n.iter = 10000, DIC = TRUE
)
```
```{r clean3, echo=FALSE, purl=FALSE}
removefiles <- c(removefiles, model.loc)
```

## Autoregressive AR(1) time series models {#sec-jags-ar1}

A variation of the random walk model is the autoregressive time series model of order 1, AR(1). This model introduces a coefficient, which we will call $b$. The parameter $b$ controls the degree to which the random walk reverts to the mean---when $b = 1$, the model is identical to the random walk, but at smaller values, the model will revert back to the mean (which in this case is zero). Also, $b$ can take on negative values.

The math to describe the AR(1) time series model is:

\begin{equation}
x_t = b \, x_{t-1} + u + w_t, \text{ where }w_t \sim \N(0,q)
(\#eq:jags-obs-eqn-ar1)
\end{equation}
Now $E[x_t] = b \, x_{t-1} + u$.

Once again we need to put a prior on $x_0$, which appears in $E[x_1]$. An AR(1) with $|b|<1$ is a stationary process and the variance of the stationary distribution of $x_t$ is $q/(1-b^2)$. If you think that $x_0$ has the stationary distribution (does your data look stationary?) then you can use the variance of the stationary distribution of $x_t$ for your prior. We specify priors with the precision (1 over the variance) instead of the variance. `inv.q` is $1/q$. Thus the precision of the stationary distribution of $x_0$ is `inv.q * (1-b*b)`.

```{r jags-ar1est, echo=TRUE, results='hide', cache=TRUE}
# AR(1) MODEL WITH AND ESTIMATED AR COEFFICIENT

model.loc <- ("ar1_intercept.txt")
jagsscript <- cat("
model {  
   u ~ dnorm(0, 0.01); 
   inv.q ~ dgamma(0.001,0.001); 
   q <- 1/inv.q; 
   b ~ dunif(-1,1);
   
   X0 ~ dnorm(0, inv.q * (1 - b * b));
   X[1] ~ dnorm(b * X0 + u, inv.q);
   for(i in 2:N) {
      X[i] ~ dnorm(b * X[i-1] + u, inv.q);
   }
}  
", file = model.loc)

jags.data <- list("X" = Wind, "N" = N)
jags.params <- c("q", "u", "b")
mod_ar1_intercept <- R2jags::jags(jags.data,
  parameters.to.save = jags.params,
  model.file = model.loc, n.chains = 3, n.burnin = 5000, n.thin = 1,
  n.iter = 10000, DIC = TRUE
)
```
```{r clean4, echo=FALSE, purl=FALSE}
removefiles <- c(removefiles, model.loc)
```

## Regression with AR(1) errors {#sec-jags-regression-with-corr-errors2}

The AR(1) model in the previous section suggests a way that we could include correlated errors in our linear regression. We could use the $x_t$ AR(1) process as our errors for $y_t$. Here is an example of modifying the intercept only linear regression model. We will set $u$ to 0 so that our AR(1) errors have a mean of 0.

\begin{equation}
\begin{gathered}
x_t = b \, x_{t-1} + w_t, \text{ where }w_t \sim \N(0,q) \\
y_t = a + x_t
\end{gathered}
(\#eq:jags-obs-eqn-lmar2)
\end{equation}

The problem with this is that we need a distribution for $y_t$. We cannot use `Y[i] <- a + X[i]` in our JA
GS code. So we need to re-write this as $y_t \sim N(a + b \, x_{t-1}, q)$.

\begin{equation}
\begin{gathered}
y_t \sim N(a + b \, x_{t-1}, q) \\
x_t = y_t - a
\end{gathered}
(\#eq:jags-obs-eqn-lmar2b)
\end{equation}

We will create the variable `EY` so we can keep track of our $y_t$ predictions, conditioned on $t-1$.

```{r jags-lm1ar2, results='hide', cache=TRUE}
# LINEAR REGRESSION with autocorrelated errors
# no covariates, intercept only.

model.loc <- ("lm_intercept_ar1b.txt")
jagsscript <- cat("
model {  
   a ~ dnorm(0, 0.01); 
   inv.q ~ dgamma(0.001,0.001); 
   q <- 1/inv.q; 
   b ~ dunif(-1,1);
   
   X0 ~ dnorm(0, inv.q * (1 - b * b));
   # t=1
   EY[1] = a + b * X0;
   Y[1] ~ dnorm(EY[1], inv.q);
   X[1] <- Y[1] - a;
   for(i in 2:N) {
      EY[i] = a + b * X[i-1];
      Y[i] ~ dnorm(EY[1], inv.q);
      X[i] <- Y[i]-a;
   }
}  
", file = model.loc)

jags.data <- list("Y" = Wind, "N" = N)
jags.params <- c("q", "EY", "a", "b")
mod_ar1_intercept <- R2jags::jags(jags.data,
  parameters.to.save = jags.params,
  model.file = model.loc, n.chains = 3, n.burnin = 5000, n.thin = 1,
  n.iter = 10000, DIC = TRUE
)
```
```{r clean2, include=FALSE, purl=FALSE}
removefiles <- c(removefiles, model.loc)
```



## Univariate state space model {#sec-jags-uss}

Now we will combine the process and observation models to create a univariate state-space model. This is the classic stochastic level model.

\begin{equation}
\begin{gathered}
x_t = x_{t-1} + u + w_t, \, w_t \sim N(0,q)\\
y_t = x_t + v_t, \, v_t \sim \N(0, r)
\end{gathered}
(\#eq:jags-proc-eqn-uss)
\end{equation}

For the process model, there are a number of ways to parameterize the first state ($x_1$). Because $x$ is a random walk model not a stationary AR(1), we will place a vague weakly informative prior on $x_0$: $x_0 \sim \N(y_1, \sigma=10)$. Note, because we use $Y_1$, our first data point should not be NA. `EY` is added so that we can track the model fits for $y$. In this case it is just `X` but in more complex models it will involve more parameters.

```{r jags-ss1, echo=TRUE, results='hide', cache=TRUE}
# 5. MAKE THE SS MODEL for a stochastic level model

model.loc <- ("ss_model.txt")
jagsscript <- cat("
model {  
   # priors on parameters
   u ~ dnorm(0, 0.01); 
   inv.q ~ dgamma(0.001,0.001); 
   q <- 1/inv.q;
   inv.r ~ dgamma(0.001,0.001);
   r <- 1/inv.r; 

   X0 ~ dnorm(0, 0.001);
   X[1] ~ dnorm(X0 + u, inv.q);
   EY[1] <- X[1];
   Y[1] ~ dnorm(EY[1], inv.r);
   for(i in 2:N) {
      X[i] ~ dnorm(X[i-1] + u, inv.q);
      EY[i] <- X[i];
      Y[i] ~ dnorm(EY[i], inv.r); 
   }
}  
", file = model.loc)
```


```{r jags-ss1-fit, echo=TRUE, results='hide', cache=TRUE}
jags.data <- list("Y" = Wind, "N" = N)
jags.params <- c("q", "r", "EY", "u")
mod_ss <- jags(jags.data,
  parameters.to.save = jags.params, model.file = model.loc, n.chains = 3,
  n.burnin = 5000, n.thin = 1, n.iter = 10000, DIC = TRUE
)
```

```{r clean5, echo=FALSE, purl=FALSE}
removefiles <- c(removefiles, model.loc)
```

## Poisson observation errors {#sec-jags-poisson}
  
So far we have used the following observation model $y_t \sim \N(x_t, r)$. 
We can change this to a Poisson observation error model:
$y_t \sim \text{Pois}(\lambda_t)$ where $E[y_t] = \lambda_t$. $\text{log}(\lambda_t) = x_t$ where $x_t$ is our process model. 

All we need to change to allow Poisson errors is to change the `Y[i]` part to
```
log(EY[i]) <- X[i]
Y[i] ~ dpois(EY[i])
```
We also need to ensure that our data are 
integers and we remove the `r` part since the Poisson does not have that.

Our univariate state-space code for Poisson observation errors is the following:

```{r jags-ss1-pois, echo=TRUE, results='hide'}
# SS MODEL with Poisson errors

model.loc <- ("ss_model_pois.txt")
jagsscript <- cat("
model {  
   # priors on parameters
   u ~ dnorm(0, 0.01); 
   inv.q ~ dgamma(0.001,0.001); 
   q <- 1/inv.q;

   X0 ~ dnorm(0, 0.001);
   X[1] ~ dnorm(X0 + u, inv.q);
   log(EY[1]) <- X[1]
   Y[1] ~ dpois(EY[1])
   for(i in 2:N) {
      X[i] ~ dnorm(X[i-1] + u, inv.q);
      log(EY[i]) <- X[i]
      Y[i] ~ dpois(EY[i]); 
   }
}  
", file = model.loc)
```

We will fit this to the wild dogs data in the **MARSS** package. When we use a Gaussian error model with population data, we would normally log the data, so that still $E[log(y_t)]) = f(x_t)$. In the Poisson model, that log relationship still exists because $log(E[y_t]) = x_t$ but we pass in the raw count data not the log of that. Note, $E[log(y_t)]$ and $log(E[y_t])$ are not equivalent so you will see some difference with the Poisson fits.

```{r jags-ss1-fit-pois, echo=TRUE, results='hide'}
data(wilddogs, package="MARSS")
jags.data <- list("Y" = wilddogs[,2], "N" = nrow(wilddogs))
jags.params <- c("q", "EY", "u")
mod_ss <- jags(jags.data,
  parameters.to.save = jags.params, model.file = model.loc, n.chains = 3,
  n.burnin = 5000, n.thin = 1, n.iter = 10000, DIC = TRUE
)
```

```{r clean7, echo=FALSE, purl=FALSE}
removefiles <- c(removefiles, model.loc)
```

## Forecasting with JAGS models {#sec-jags-forecast}

There are a number of different approaches to using Bayesian time series models to perform forecasting. One approach might be to fit a model, and use those posterior distributions to forecast as a secondary step (say within R). A more streamlined approach is to do this within the JAGS code itself. We can take advantage of the fact that JAGS allows you to include NAs in the response variable (but never in the predictors). Let's use the same Wind dataset, and the univariate state-space model described above to forecast three time steps into the future. We can do this by including 3 more NAs in the dataset, and incrementing the variable ```N``` by 3.

```{r jags-cov-forecast, results='hide', cache=TRUE}
jags.data <- list("Y" = c(Wind, NA, NA, NA), "N" = (N + 3))
jags.params <- c("q", "r", "EY", "u")
model.loc <- ("ss_model.txt")
mod_ss_forecast <- jags(jags.data,
  parameters.to.save = jags.params,
  model.file = model.loc, n.chains = 3, n.burnin = 5000, n.thin = 1,
  n.iter = 10000, DIC = TRUE
)
```

We can inspect the fitted model object, and see that ```EY``` contains the 3 new predictions for the forecasts from this model. 


\clearpage

## Problems {#sec-jags-problems}


1. Fit the intercept only model from section \@ref(sec-jags-lr-no-covariates). Set the burn-in to 3, and when the  model completes, plot the time series of the parameter ```u``` for the first MCMC chain.

    a. Based on your visual inspection, has the MCMC chain convered?
    
    b. What is the ACF of the first MCMC chain?

2. Increase the MCMC burn-in for the model in question 1 to a value that you think is reasonable. After the model has converged, calculate the Gelman-Rubin diagnostic for the fitted model object.

3. Compare the results of the ```plotModelOutput()``` function for the intercept only model from section \@ref(sec-jags-lr-no-covariates). You will to add "predY" to your JAGS model and to the list of parameters to monitor, and re-run the model.

5. Plot the posterior distribution of $b$ for the AR(1) model in section \@ref(sec-jags-ar1).  Can this parameter be well estimated for this dataset?

6. Plot the posteriors for the process and observation variances (not standard deviation) for the univariate state-space model in section \@ref(sec-jags-uss). Which is larger for this dataset?

7. Add the effect of temperature to the AR(1) model in section \@ref(sec-jags-ar1).  Plot the posterior for ```C``` and compare to the posterior for ```C``` from the model in section \@ref(sec-jags-covariates).

8. Plot the fitted values from the model in section \@ref(sec-jags-forecast), including the forecasts, with the 95\% credible intervals for each data point.

9. The following is a dataset from the Upper Skagit River (Puget Sound, 1952-2005) on salmon spawners and recruits:
    ```{r jags-hwdata, echo=TRUE}
Spawners <- c(2662, 1806, 1707, 1339, 1686, 2220, 3121, 5028, 9263, 4567, 1850, 3353, 2836, 3961, 4624, 3262, 3898, 3039, 5966, 5931, 7346, 4911, 3116, 3185, 5590, 2485, 2987, 3829, 4921, 2348, 1932, 3151, 2306, 1686, 4584, 2635, 2339, 1454, 3705, 1510, 1331, 942, 884, 666, 1521, 409, 2388, 1043, 3262, 2606, 4866, 1161, 3070, 3320)
Recruits <- c(12741, 15618, 23675, 37710, 62260, 32725, 8659, 28101, 17054, 29885, 33047, 20059, 35192, 11006, 48154, 35829, 46231, 32405, 20782, 21340, 58392, 21553, 27528, 28246, 35163, 15419, 16276, 32946, 11075, 16909, 22359, 8022, 16445, 2912, 17642, 2929, 7554, 3047, 3488, 577, 4511, 1478, 3283, 1633, 8536, 7019, 3947, 2789, 4606, 3545, 4421, 1289, 6416, 3647)
logRS <- log(Recruits / Spawners)
    ```

    a. Fit the following Ricker model to these data using the following linear form of this model with normally distributed errors:   
\begin{equation*}
log(R_t/S_t) = a + b \times S_t + e_t,\text{ where } e_t \sim \N(0,\sigma^2)
\end{equation*}
    You will recognize that this form is exactly the same as linear regression, with independent errors (very similar to the intercept only model of Wind we fit in section \@ref(sec-jags-lr-no-covariates)).

    b. Within the constraints of the Ricker model, think about other ways you might want to treat the errors. The basic model described above has independent errors that are not correlated in time. Approaches to analyzing this dataset might involve

        *  modeling the errors as independent (as described above)
        
        *  modeling the errors as autocorrelated
        
        *  fitting a state-space model, with independent or correlated process errors

    Fit each of these models, and compare their performance (either using their predictive ability, or forecasting ability).


```{r jags-reset, include=FALSE, purl=FALSE}
file.remove(removefiles)
```
