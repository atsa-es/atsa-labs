```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=FALSE)
```

# Using DLMs to model changing seasonality {#chap-seasonal-dlm}
\chaptermark{Changing season}

As discussed in Section \@ref(sec-msscov-season-fourier) in the covariates chapter, we can model season with sine and cosine covariates.

$$y_t = x_t + \beta_1 \sin(2 \pi t/p) + \beta_2 \cos(2 \pi t/p) + e_t$$
where $t$ is the time step (1 to length of the time series) and $p$ is the frequency of the data (e.g. 12 for monthly data). $\alpha_t$ is the mean level about which the data $y_t$ are fluctuating.

We can simulate data like this as follows:

```{r}
set.seed(1234)
TT <- 100
q <- 0.1; r <- 0.1
beta1 <- 0.6
beta2 <- 0.4
cov1 <- sin(2*pi*(1:TT)/12)
cov2 <- cos(2*pi*(1:TT)/12)
xt <- cumsum(rnorm(TT,0,q))
yt <- alphat + beta1*cov1 + beta2*cov2 + rnorm(TT,0,r)
plot(yt, type="l", xlab="t")
```

In this case, the seasonal cycle is constant over time since $\beta_1$ and $\beta_2$ are fixed (not varying in time).

```{r echo=FALSE}
plot(1:12,(beta1*cov1 + beta2*cov2)[1:12],xlab="month",ylab="monthly effect")
title("seasonal cycle")
```

The $\beta$ determine the shape and amplitude of the seasonal cycle---though in this case we will only have one peak per year.

```{r echo=FALSE}
require(ggplot2)
df <- data.frame()
for(beta1 in c(4,0.1))
  for(beta2 in c(-4,0.2)){
    tmp <- data.frame(line = paste0("b1=", beta1, " b2=",beta2), beta1 = beta1, beta2 = beta2, 
    value = beta1*sin(2*pi*(1:12)/12)+beta2*cos(2*pi*(1:12)/12), month=1:12)
    df <- rbind(df, tmp)
  }
df$line <- as.factor(df$line)
ggplot(df, aes(x=month, y=value, color=line)) + geom_line()
```

## Time-varying season

If $\beta_1$ and $\beta_2$ vary in time then the seasonal cycle also varies in time. Let's imagine that $\beta_1$ varies from -1 to 1 over our 100 time steps while $\beta_2$ varies from 1 to -1.

So the seasonal cycle flips between the start and end of our time series.

```{r echo=FALSE}
require(ggplot2)
df <- data.frame()
beta1s = seq(-1,1,2/99)
beta2s = seq(1,-1,-2/99)
for(t in c(1,25,50,100)){
  beta1 = beta1s[t]
  beta2 = beta2s[t]
  tmp <- data.frame(t=paste("t =",t), line = paste0("b1=", beta1, " b2=",beta2), beta1 = beta1, beta2 = beta2, 
    value = beta1*sin(2*pi*(1:12)/12)+beta2*cos(2*pi*(1:12)/12), month=1:12)
    df <- rbind(df, tmp)
}
df$t = as.factor(df$t)
ggplot(df, aes(x=month, y=value)) + 
  geom_line() + facet_wrap(~t) +
  ggtitle("Seasonal cycle at different times")
```

A time series simulated with that flip looks like so:

```{r echo=FALSE}
set.seed(1234)
TT <- 100
q <- 0.1; r <- 0.1
beta1 <- seq(-1,1,2/99)
beta2 <- seq(1,-1,-2/99)
cov1 <- sin(2*pi*(1:TT)/12)
cov2 <- cos(2*pi*(1:TT)/12)
xt <- cumsum(rnorm(TT,0,q))
yt <- alphat + beta1*cov1 + beta2*cov2 + rnorm(TT,0,r)
plot(yt, type="l", xlab="t")
```

## Using DLMs to estimate changing season


Here is the data model BUT the $y$ will be demeaned. Each sensor observes $a$ plus their own independent local AR-1 trend. Notice no $v_t$. The model error comes through the AR-1 $x$ processes.

$$\begin{bmatrix}y1 \\ y2 \\ y3\end{bmatrix} = \begin{bmatrix}1&1&0&0 \\ 1&0&1&0 \\ 1&0&0&1\end{bmatrix} \begin{bmatrix}a \\ x1 \\ x2 \\ x3\end{bmatrix}_t$$

## Fit the model

We specify this one-to-one in R for `MARSS()`:

```{r mod.list1}
makemod <- function(n){
  B <- matrix(list(0), n+1, n+1)
diag(B)[2:(n+1)] <- paste0("b", 1:n)
B[1,1] <- 1
A <- "zero"
Z <- cbind(1,diag(1,n))
Q <- matrix(list(0),n+1,n+1)
Q[1,1] <- 1
diag(Q)[2:(n+1)] <- paste0("q",1:n)
R <- "zero"
U <- "zero"
x0 <- "zero"
mod.list <- list(B=B, A=A, Z=Z, Q=Q, R=R, U=U, x0=x0, tinitx=0)
return(mod.list)
}
mod.list1 <- makemod(3)
```

Demean the data.
```{r fit.mod1}
dat2 <- dat - apply(dat,1,mean) %*% matrix(1,1,TT)
```

Fit to that
```{r}
fit.mod1 <- MARSS(dat2, model=mod.list1)
```

## Show the fits

X1 is the estimate of the signal. The mean has been removed. X2, X3 and X4 are the AR-1 errors for our sensors.

```{r}
require(ggplot2)
autoplot(fit.mod1, plot.type="xtT", conf.int=FALSE)
```


## Estimated common trend from state-space model

```{r echo=FALSE}
t <- 1:TT
df <- data.frame(val=c(fit.mod1$states[1,],
                 signal,
                 apply(dat2,2,mean)),
                 name=rep(c("estimate","true signal","mean data"),each=TT),
                 x=rep(t, 3))
rmse <- sqrt(mean((fit.mod1$states[1,] - signal)^2))

ggplot(subset(df, name!="mean data"),aes(y = val, x = x, color = name)) + 
  geom_line(size=1.2) + ggtitle(paste0("RMSE = ", rmse))
```

## Compare the common trend to mean of data 

You could just take the average of the 3 sensors assuming they were independent with similar error levels.  With some of the sensors being really bad, this would not give a good estimate of the signal.  The model allowed us to estimate $b$ for each sensor and $q$ (variance) thus allowing us to estimate how much to weight each sensor.

```{r echo=FALSE}
ggplot(df, aes(y = val, x = x, color = name)) + 
  geom_line(size=1.2)
```

## Missing data

One nice features of this approach is that it is robust to a fair bit of missing data. Here I delete a third of the data. I do this randomly throughout the dataset.  The data look pretty hopeless. No signal to be seen.

```{r echo=FALSE}
dat.miss <- dat
dat.miss[sample(n*TT,n*TT/3)] <- NA
dat2.miss <- dat.miss - apply(dat.miss,1,mean,na.rm=TRUE) %*% matrix(1,1,TT)
df <- data.frame(val=as.vector(t(dat.miss)),
                 name=rep(rownames(dat.miss),each=TT),
                 x=rep(t, 3))
ggplot(df, aes(y = val, x = x)) + 
  geom_line(size=1.2) + facet_wrap(~name)
```

Fit as usual:
```{r}
fit <- MARSS(dat2.miss, model=mod.list1, silent=TRUE)
```

But though we can't see the signal in the data, it is there.
```{r echo=FALSE}
df <- data.frame(val=c(fit$states[1,],
                 signal,
                 apply(dat2.miss,2,mean, na.rm=TRUE)),
                 name=rep(c("estimate","true signal","mean data"),each=TT),
                 x=rep(1:TT, 3))
rmse <- sqrt(mean((fit$states[1,] - signal)^2))

ggplot(subset(df, name!="mean data"),aes(y = val, x = x, color = name)) + 
  geom_line(size=1.2) + ggtitle(paste0("RMSE = ", rmse))
```

Averaging our sensors doesn't work since there are so many missing values and we will have missing values in our average.

```{r echo=FALSE}
ggplot(df,aes(y = val, x = x, color = name)) + 
  geom_line(size=1.2)
```

Another type of missing data are strings of missing data. Here I create a data set with random strings of missing values. Again the data look really hopeless and definitely cannot average across the data since we'd be averaging across different data sets.
```{r echo=FALSE}
dat.miss <- dat
for(i in 1:n)
dat.miss[i, arima.sim(TT, model=list(ar=.8)) < -1] <- NA
dat2.miss <- dat.miss - apply(dat.miss,1,mean,na.rm=TRUE) %*% matrix(1,1,TT)
df <- data.frame(val=as.vector(t(dat.miss)),
                 name=rep(rownames(dat.miss),each=TT),
                 x=rep(t, 3))
ggplot(df, aes(y = val, x = x)) + 
  geom_line(size=1.2) + facet_wrap(~name)
```

We can fit as usual and see that it is possible to recover the signal.
```{r}
fit <- MARSS(dat2.miss, model=mod.list1, silent=TRUE)
```

```{r echo=FALSE}
t <- 1:TT
df <- data.frame(val=c(fit$states[1,],
                 signal,
                 apply(dat2.miss,2,mean, na.rm=TRUE)),
                 name=rep(c("estimate","true signal","mean data"),each=TT),
                 x=rep(t, 3))
rmse <- sqrt(mean((fit$states[1,] - signal)^2))

ggplot(subset(df, name!="mean data"),aes(y = val, x = x, color = name)) + 
  geom_line(size=1.2) + ggtitle(paste0("RMSE = ", rmse))

```

## Correlated noise

In the simulated data, the AR-1 errors were uncorrelated. Each error time series was independent of the others. But we might want to test a model where the errors are correlated. The processes that drive variability in sensors can sometimes be a factor that are common across all our sensors, like say average wind speed or rainfall.

Our AR-1 errors would look like so with covariance $c$.

$$\begin{bmatrix}e \\ w_1 \\ w_2 \\ w_3\end{bmatrix}_t, \quad \begin{bmatrix}e \\ w_1 \\ w_2 \\ w_3\end{bmatrix}_t \sim MVN\left(0, \begin{bmatrix}1&0&0&0\\0&q_1&c_1&c_2 \\ 0&c_1&q_2&c_3 \\ 0&c_2&c_3&q_3\end{bmatrix}\right)$$
 To fit this model, we need to create a $Q$ matrix that looks like the above.  It's a bit of a hassle.
 
```{r}
Q <- matrix(list(0),n+1,n+1)
Q[1,1] <- 1
Q2 <- matrix("q",n,n)
diag(Q2) <- paste0("q", 1:n)
Q2[upper.tri(Q2)] <- paste0("c",1:n)
Q2[lower.tri(Q2)] <- paste0("c",1:n)
Q[2:(n+1),2:(n+1)] <- Q2
Q
```

Now we can fit as usual using this $Q$ in our model list.

```{r}
mod.list2 <- mod.list1
mod.list2$Q <- Q
fit <- MARSS(dat2, model=mod.list2)
```

The AIC is larger indicating that this model is not more supported, which is not surprising given that the data are not correlated with each other.
```{r}
c(fit$AIC, fit.mod1$AIC)
```

```{r echo=FALSE}
df <- data.frame(val=c(fit$states[1,],
                 signal,
                 apply(dat2,2,mean, na.rm=TRUE)),
                 name=rep(c("estimate","true signal","mean data"),each=TT),
                 x=rep(t, 3))
rmse <- sqrt(mean((fit$states[1,] - signal)^2))

ggplot(subset(df, name!="mean data"),aes(y = val, x = x, color = name)) + 
  geom_line(size=1.2) + ggtitle(paste0("RMSE = ", rmse))
```



## Discussion

This example worked because I had a sensor that was quite a bit better than the others with a much smaller level of observation error variance (sd=1 versus 28 and 41 for the others). I didn't know which one it was, but I did have at least one good sensor. If I up the observation error variance on the first (good) sensor, then my signal estimate is not so good. The variance of the signal estimate is better than the average, but it is still bad. There is only so much that can be done when the sensor adds so much error.

```{r}
sd <- sqrt(c(10, 28, 41))
dat[1,] <- signal + arima.sim(TT, model=list(ar=ar[1]), sd=sd[1])
dat2 <- dat - apply(dat,1,mean) %*% matrix(1,1,TT)

fit <- MARSS(dat2, model=mod.list1, silent=TRUE)
```

```{r echo=FALSE}
df <- data.frame(val=c(fit$states[1,],
                 signal,
                 apply(dat2,2,mean)),
                 name=rep(c("estimate","true signal","mean data"),each=TT),
                 x=rep(t, 3))
rmse <- sqrt(mean((fit$states[1,] - signal)^2))
ggplot(df ,aes(y = val, x = x, color = name)) + 
  geom_line(size=1.2) + ggtitle(paste0("3 bad sensors. RMSE = ", rmse))
```

One solution is to have more sensors.  They can all be horrible but now that I have more, I can get a better estimate of the signal.  In this example I have 12 bad sensors instead of 3.  The properties of the sensors are the same as in the example above. I will add the new data to the existing data.

```{r}
set.seed(123)
datm <- dat
for (i in 1:2){
tmp <- createdata(n, TT, ar, sd)
datm <- rbind(datm, tmp$dat)
}
datm2 <- datm - apply(datm,1,mean) %*% matrix(1,1,TT)

fit <- MARSS(datm2, model=makemod(dim(datm2)[1]), silent=TRUE)
```

```{r echo=FALSE}
rmse <- sqrt(mean((fit$states[1,] - signal)^2))
df <- data.frame(val=c(fit$states[1,],
                 signal,
                 apply(dat2,2,mean)),
                 name=rep(c("estimate","true signal","mean data"),each=TT),
                 x=rep(t, 3))
ggplot(df ,aes(y = val, x = x, color = name)) + 
  geom_line(size=1.2) + ggtitle(paste0(dim(datm2)[1], " bad sensors. RMSE = ", rmse))
```

Some more caveats are that I simulated data that was the same as the model that I fit, except the signal. However an AR-1 with $b$ and $q$ (sd) estimated is quite flexible and this will likely work for data that is roughly AR-1.  A common exception is very smooth data that you get from sensors that record dense data (like every second).  That kind of sensor data may need to be subsampled (every 10 or 20 or 30 data point) to get AR-1 like data.  

Lastly I set the seed to 1234 to have an example that looks *ok*. If you comment that out and rerun the code, you'll quickly see that the example I used is not one of the bad ones. It's not unusually good, just not unusually bad. 

On the otherhand, I poised a difficult problem with two quite awful sensors. A sensor with a random walk error would be really alarming and hopefully you would not have that type of error.  But you might.  IT can happen when local conditions are undergoing a random walk with slow reversion to the mean. Many natural systems look like that.  If you have that problem, subsampling that *random walk* sensor might be a good idea.
